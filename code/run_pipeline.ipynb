{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/adikan2k/Final-Project-Group-LexiCore.git\n",
        "\n",
        "%cd /content/Final-Project-Group-LexiCore\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGC7ArYH3DFG",
        "outputId": "5cd534be-1d6a-4a28-c897-c144c2a80340"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Final-Project-Group-LexiCore'...\n",
            "remote: Enumerating objects: 450, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 450 (delta 31), reused 25 (delta 13), pack-reused 384 (from 2)\u001b[K\n",
            "Receiving objects: 100% (450/450), 319.90 MiB | 27.75 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n",
            "/content/Final-Project-Group-LexiCore\n",
            " code\t\t\t    'Final Group Project Report'    Outputs\n",
            " data\t\t\t     Final-Project-Group-LexiCore   README.md\n",
            "'Final Group Presentation'   LICENSE\t\t\t    run_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_pipeline.py\n",
        "\"\"\"\n",
        "Final Pipeline Orchestrator\n",
        "\n",
        "This script:\n",
        "  1. Loads the cleaned corpus (output of Day 1+2).\n",
        "  2. Runs retrieval over title + abstract.\n",
        "  3. Calls the summarization engine (generate_summaries).\n",
        "  4. Saves a final digest JSON.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "ROOT = Path(__file__).resolve().parent\n",
        "\n",
        "\n",
        "\n",
        "ROOT = Path(__file__).resolve().parent\n",
        "CODE_DIR = ROOT / \"code\"\n",
        "sys.path.append(str(CODE_DIR))\n",
        "\n",
        "try:\n",
        "\n",
        "    from summarization import generate_summaries  # type: ignore\n",
        "except Exception:\n",
        "\n",
        "    def generate_summaries(text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Fallback summary used only if real summarization.py is not available.\"\"\"\n",
        "        snippet = (text or \"\").strip()\n",
        "        one_sentence = snippet[:250]\n",
        "        return {\n",
        "            \"one_sentence\": one_sentence,\n",
        "            \"three_sentence\": one_sentence,\n",
        "            \"five_bullets\": [\n",
        "                f\"- {one_sentence}\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "def load_corpus() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load cleaned paper corpus produced by Day 1/2.\n",
        "\n",
        "    Tries both:\n",
        "      data/processed/cleaned_paper.parquet\n",
        "      data/processed/cleaned_papers.parquet\n",
        "    because naming sometimes differs.\n",
        "    \"\"\"\n",
        "    base = ROOT / \"data\" / \"processed\"\n",
        "    candidates = [\n",
        "        base / \"cleaned_paper.parquet\",\n",
        "        base / \"cleaned_papers.parquet\",\n",
        "    ]\n",
        "    last_err = None\n",
        "    for path in candidates:\n",
        "        try:\n",
        "            print(f\"Trying to load: {path}\")\n",
        "            df = pd.read_parquet(path)\n",
        "            print(f\"Loaded {len(df)} rows from {path.name}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not load cleaned parquet. Tried {', '.join(str(c) for c in candidates)}.\\n\"\n",
        "        f\"Last error: {last_err}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def simple_retrieval(query: str, df: pd.DataFrame, top_k: int = 5) -> List[int]:\n",
        "    \"\"\"\n",
        "    Very simple keyword retrieval over title + abstract.\n",
        "\n",
        "    (If you later want to plug in BM25/FAISS hybrid, replace this\n",
        "     with calls to your Day-2 retrieval engine.)\n",
        "    \"\"\"\n",
        "    text_series = (\n",
        "        df.get(\"title\", \"\").fillna(\"\").astype(str)\n",
        "        + \" \"\n",
        "        + df.get(\"original_abstract\", df.get(\"abstract\", \"\")).fillna(\"\").astype(str)\n",
        "    )\n",
        "\n",
        "    mask = text_series.str.contains(query, case=False, na=False)\n",
        "    indices = df[mask].index.tolist()\n",
        "\n",
        "    if not indices:\n",
        "        print(\"No exact keyword matches found; falling back to first top_k papers.\")\n",
        "        indices = df.index.tolist()\n",
        "\n",
        "    return indices[:top_k]\n",
        "\n",
        "\n",
        "def build_digest(query: str, df: pd.DataFrame, top_k: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Build digest object:\n",
        "      - retrieval to get top_k docs\n",
        "      - summarization for each abstract\n",
        "    \"\"\"\n",
        "    indices = simple_retrieval(query, df, top_k)\n",
        "    papers_out = []\n",
        "\n",
        "    for rank, idx in enumerate(indices, start=1):\n",
        "        row = df.loc[idx]\n",
        "        abstract = row.get(\"original_abstract\", row.get(\"abstract\", \"\"))\n",
        "\n",
        "        summaries = generate_summaries(str(abstract))\n",
        "\n",
        "        papers_out.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"paper_id\": row.get(\"paper_id\", int(idx)),\n",
        "                \"title\": row.get(\"title\", \"Untitled\"),\n",
        "                \"venue\": row.get(\"venue\"),\n",
        "                \"year\": int(row[\"year\"]) if \"year\" in row and pd.notna(row[\"year\"]) else None,\n",
        "                \"summaries\": summaries,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"num_results\": len(papers_out),\n",
        "        \"papers\": papers_out,\n",
        "    }\n",
        "\n",
        "\n",
        "def save_digest(digest: Dict[str, Any], output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Save digest to JSON, converting numpy/pandas types to plain Python types.\n",
        "    \"\"\"\n",
        "    folder = os.path.dirname(output_path)\n",
        "    if folder:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    def convert(o):\n",
        "        if isinstance(o, (np.integer,)):\n",
        "            return int(o)\n",
        "        if isinstance(o, (np.floating,)):\n",
        "            return float(o)\n",
        "        if isinstance(o, np.ndarray):\n",
        "            return o.tolist()\n",
        "        return str(o)\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(digest, f, indent=2, default=convert)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    query: str,\n",
        "    top_k: int = 5,\n",
        "    output_path: str = \"Outputs/pipeline_digest.json\",\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline:\n",
        "      1. Load corpus\n",
        "      2. Retrieve top_k docs\n",
        "      3. Summarize each\n",
        "      4. Save digest JSON\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    print(f\" Running pipeline for query: '{query}'\")\n",
        "    print(f\"   top_k       = {top_k}\")\n",
        "    print(f\"   output_path = {output_path}\")\n",
        "\n",
        "    print(\"\\n[1] Loading corpus...\")\n",
        "    df = load_corpus()\n",
        "    print(f\"    Loaded {len(df)} papers.\")\n",
        "\n",
        "    print(\"\\n[2] Building digest (retrieval + summarization)...\")\n",
        "    digest = build_digest(query, df, top_k=top_k)\n",
        "\n",
        "    print(\"\\n[3] Saving digest JSON...\")\n",
        "    save_digest(digest, output_path)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"\\n Done in {elapsed:.2f} seconds.\")\n",
        "    print(f\"   Papers in digest: {digest['num_results']}\")\n",
        "    print(f\"   JSON saved to:    {output_path}\")\n",
        "\n",
        "    return digest\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Run Day 3 digest pipeline.\")\n",
        "    parser.add_argument(\n",
        "        \"--query\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Search query, e.g. 'transformer models for NLP'\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--top_k\",\n",
        "        type=int,\n",
        "        default=5,\n",
        "        help=\"Number of papers to include in the digest\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        type=str,\n",
        "        default=\"Outputs/pipeline_digest.json\",\n",
        "        help=\"Output path for JSON digest\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run_pipeline(\n",
        "        query=args.query,\n",
        "        top_k=args.top_k,\n",
        "        output_path=args.output,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybUDweHY3Occ",
        "outputId": "3a1a4623-4763-45c4-f43e-19531bafbfcd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"Outputs/transformer_pipeline_digest.json\", \"r\") as f:\n",
        "    digest = json.load(f)\n",
        "\n",
        "print(\"Query:\", digest[\"query\"])\n",
        "print(\"Num results:\", digest[\"num_results\"])\n",
        "print(\"\\nTop 2 paper titles:\")\n",
        "\n",
        "for p in digest[\"papers\"][:2]:\n",
        "    print(f\"- #{p['rank']} {p['title']}\")\n",
        "    if isinstance(p[\"summaries\"], dict):\n",
        "        print(\"  1-sentence summary:\", p[\"summaries\"].get(\"one_sentence\", \"\")[:200])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T30cvYoC3zV5",
        "outputId": "a429fa64-66f0-4926-dc25-4c4d82d0099f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: transformer models\n",
            "Num results: 3\n",
            "\n",
            "Top 2 paper titles:\n",
            "- #1 Controlling changes to attention logits\n",
            "  1-sentence summary: Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying \n",
            "\n",
            "- #2 IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference\n",
            "  1-sentence summary: Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu rank-bm25 sentence-transformers --quiet"
      ],
      "metadata": {
        "id": "y-lu_j646liF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "print(\"FAISS version:\", faiss.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czF_uKct613Z",
        "outputId": "6363da1b-81ac-4b32-941c-5c2191196ca4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS version: 1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class BM25Retriever:\n",
        "    def __init__(self, corpus=None):\n",
        "        self.tokenized_corpus = None\n",
        "        self.bm25 = None\n",
        "        self.corpus = corpus\n",
        "        if corpus is not None:\n",
        "            self.tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "            self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "        tokenized_query = query.lower().split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(int(idx), float(scores[idx])) for idx in top_indices]\n",
        "\n",
        "\n",
        "class FAISSRetriever:\n",
        "    def __init__(self, embeddings=None, encoder_model=\"all-MiniLM-L6-v2\"):\n",
        "        self.encoder = SentenceTransformer(encoder_model)\n",
        "        self.index = None\n",
        "        self.dimension = None\n",
        "        if embeddings is not None:\n",
        "            self._build_index(embeddings)\n",
        "\n",
        "    def _build_index(self, embeddings):\n",
        "        self.embeddings = embeddings.astype(\"float32\")\n",
        "        self.dimension = self.embeddings.shape[1]\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def load_index(self, index_path):\n",
        "        self.index = faiss.read_index(index_path)\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "        if self.index is None:\n",
        "\n",
        "            return []\n",
        "        query_vec = self.encoder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "        faiss.normalize_L2(query_vec)\n",
        "        scores, indices = self.index.search(query_vec, top_k)\n",
        "        return [(int(idx), float(score)) for idx, score in zip(indices[0], scores[0])]\n",
        "\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(self, bm25_retriever, faiss_retriever, bm25_weight=0.3, semantic_weight=0.7):\n",
        "        self.bm25 = bm25_retriever\n",
        "        self.faiss = faiss_retriever\n",
        "        self.bm25_weight = bm25_weight\n",
        "        self.semantic_weight = semantic_weight\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "        bm25_res = self.bm25.search(query, top_k=50)\n",
        "        faiss_res = self.faiss.search(query, top_k=50)\n",
        "\n",
        "        bm25_scores = {idx: score for idx, score in bm25_res}\n",
        "        faiss_scores = {idx: score for idx, score in faiss_res}\n",
        "\n",
        "        all_indices = set(bm25_scores.keys()) | set(faiss_scores.keys())\n",
        "        if not all_indices:\n",
        "            return []\n",
        "\n",
        "        bm25_max = max(bm25_scores.values()) if bm25_scores else 1.0\n",
        "        faiss_max = max(faiss_scores.values()) if faiss_scores else 1.0\n",
        "\n",
        "        combined = []\n",
        "        for idx in all_indices:\n",
        "            b_score = bm25_scores.get(idx, 0.0) / bm25_max\n",
        "            f_score = faiss_scores.get(idx, 0.0) / faiss_max\n",
        "            final = self.bm25_weight * b_score + self.semantic_weight * f_score\n",
        "            combined.append((idx, final))\n",
        "\n",
        "        combined.sort(key=lambda x: x[1], reverse=True)\n",
        "        return combined[:top_k]\n",
        "\n",
        "\n",
        "class EmbeddingClassifier:\n",
        "    \"\"\"\n",
        "    Wraps Day-2 embedding-based classifier (e.g., LogisticRegression on SBERT).\n",
        "    Expects pickle with {'classifier': model, ...}.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_model=\"all-MiniLM-L6-v2\"):\n",
        "        self.encoder = SentenceTransformer(encoder_model)\n",
        "        self.classifier: LogisticRegression | None = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def attach(self, clf_obj):\n",
        "        if isinstance(clf_obj, dict):\n",
        "            self.classifier = clf_obj[\"classifier\"]\n",
        "            self.classes_ = self.classifier.classes_\n",
        "        else:\n",
        "            self.classifier = clf_obj\n",
        "            self.classes_ = self.classifier.classes_\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        embeddings = self.encoder.encode(texts, show_progress_bar=False)\n",
        "        return self.classifier.predict_proba(embeddings)\n",
        "\n",
        "    def predict_label(self, text: str):\n",
        "        probs = self.predict_proba(text)[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        label = self.classes_[top_idx]\n",
        "        confidence = float(probs[top_idx])\n",
        "        return label, confidence"
      ],
      "metadata": {
        "id": "RqcM3P1U7rUP"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline: Data Ingestion ‚Üí Preprocessing ‚Üí Embeddings\n",
    "\n",
    "**Day 1 Deliverables - Full Implementation**\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "## ‚úÖ Part 1: Data Ingestion & Normalization\n",
    "- ArXiv, ACL Anthology, S2ORC ingestion\n",
    "- Metadata normalization (paper_id, title, authors, abstract, venue, year, categories)\n",
    "- **Dataset validation** (missing abstracts, duplicates, invalid fields)\n",
    "- Output: `/data/raw/complete_dataset.parquet`\n",
    "\n",
    "## ‚úÖ Part 2: Full Preprocessing Pipeline  \n",
    "- Text cleaning, lowercasing, regex fixes\n",
    "- Tokenization, lemmatization, stopword removal\n",
    "- **Language detection**\n",
    "- **Sentence segmentation**\n",
    "- Output: `/data/processed/cleaned_papers.parquet`\n",
    "\n",
    "## ‚úÖ Part 3: Embeddings Pipeline\n",
    "- Compare Word2Vec, SBERT, and SciBERT on sample\n",
    "- Implement SBERT embedding generator\n",
    "- Generate abstract/title embeddings\n",
    "- Output: `/data/embeddings/*.npy`, `paper_index.pkl`\n",
    "\n",
    "**‚è±Ô∏è Total Runtime:** ~4-5 hours  \n",
    "**üíæ Storage Required:** ~1-2GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all required packages\n",
    "!pip install pandas arxiv requests beautifulsoup4 lxml pyarrow -q\n",
    "!pip install spacy nltk tqdm langdetect -q\n",
    "!pip install gensim sentence-transformers torch scikit-learn -q\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# data ingestion\n",
    "import arxiv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# embeddings\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# colab\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "\n",
    "print(\"‚úì Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# load spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"‚úì NLP models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Data Ingestion & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 ArXiv Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_papers(categories, max_results_per_category=300):\n",
    "    \"\"\"Fetch papers from ArXiv API.\"\"\"\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(f\"Fetching {cat}...\", end=' ')\n",
    "        search = arxiv.Search(\n",
    "            query=f'cat:{cat}',\n",
    "            max_results=max_results_per_category,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "        \n",
    "        count = 0\n",
    "        for result in client.results(search):\n",
    "            papers.append({\n",
    "                'paper_id': result.entry_id.split('/')[-1],\n",
    "                'title': result.title,\n",
    "                'authors': [author.name for author in result.authors],\n",
    "                'abstract': result.summary.replace('\\n', ' '),\n",
    "                'categories': result.categories,\n",
    "                'venue': 'arXiv',\n",
    "                'year': result.published.year,\n",
    "                'published': result.published.isoformat(),\n",
    "                'pdf_url': result.pdf_url\n",
    "            })\n",
    "            count += 1\n",
    "        \n",
    "        print(f\"{count} papers\")\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# fetch\n",
    "print(\"ArXiv Ingestion:\\n\")\n",
    "arxiv_papers = fetch_arxiv_papers(['cs.CL', 'cs.LG', 'stat.ML'], max_results_per_category=300)\n",
    "arxiv_df = pd.DataFrame(arxiv_papers)\n",
    "print(f\"\\n‚úì ArXiv: {len(arxiv_df)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ACL Anthology Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ACL\n",
    "print(\"Downloading ACL Anthology...\")\n",
    "!wget https://aclanthology.org/anthology.bib.gz -O data/raw/acl.bib.gz -q\n",
    "!gunzip -f data/raw/acl.bib.gz\n",
    "print(\"‚úì Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bibtex(filepath):\n",
    "    \"\"\"Parse BibTeX file.\"\"\"\n",
    "    papers = []\n",
    "    current = {}\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('@'):\n",
    "                if current and 'title' in current:\n",
    "                    papers.append(current)\n",
    "                parts = line[1:].split('{')\n",
    "                if len(parts) == 2:\n",
    "                    current = {'paper_id': parts[1].rstrip(',')}\n",
    "            \n",
    "            elif line.startswith('}'):\n",
    "                if current and 'title' in current:\n",
    "                    papers.append(current)\n",
    "                current = {}\n",
    "            \n",
    "            elif '=' in line and current:\n",
    "                parts = line.split('=', 1)\n",
    "                if len(parts) == 2:\n",
    "                    field = parts[0].strip()\n",
    "                    value = parts[1].strip().strip(',').strip('{}').strip('\"')\n",
    "                    current[field] = value\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# parse and normalize\n",
    "print(\"\\nParsing ACL BibTeX...\")\n",
    "acl_papers = parse_bibtex('data/raw/acl.bib')\n",
    "\n",
    "acl_normalized = []\n",
    "for p in acl_papers:\n",
    "    authors = [a.strip() for a in p.get('author', '').split(' and ')] if 'author' in p else []\n",
    "    year = None\n",
    "    try:\n",
    "        year = int(p.get('year', 0))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    acl_normalized.append({\n",
    "        'paper_id': p.get('paper_id', ''),\n",
    "        'title': p.get('title', ''),\n",
    "        'authors': authors,\n",
    "        'abstract': p.get('abstract', ''),\n",
    "        'venue': p.get('booktitle', p.get('journal', 'ACL')),\n",
    "        'year': year,\n",
    "        'url': p.get('url', '')\n",
    "    })\n",
    "\n",
    "acl_df = pd.DataFrame(acl_normalized)\n",
    "acl_df = acl_df[(acl_df['title'].str.len() > 0) & (acl_df['year'] >= 2015)]\n",
    "print(f\"‚úì ACL: {len(acl_df)} papers (2015+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 S2ORC Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_s2(query, limit=100):\n",
    "    \"\"\"Search Semantic Scholar.\"\"\"\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': min(limit, 100),\n",
    "        'fields': 'paperId,title,abstract,authors,year,venue,citationCount,fieldsOfStudy'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get('data', [])\n",
    "    except:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "# fetch\n",
    "print(\"\\nS2ORC Ingestion:\")\n",
    "queries = ['natural language processing', 'transformers bert', 'machine translation',\n",
    "           'sentiment analysis', 'named entity recognition', 'question answering']\n",
    "\n",
    "all_s2 = []\n",
    "for q in queries:\n",
    "    print(f\"  {q}...\", end=' ')\n",
    "    papers = search_s2(q, 100)\n",
    "    all_s2.extend(papers)\n",
    "    print(f\"{len(papers)} papers\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# dedupe and normalize\n",
    "seen = set()\n",
    "s2_normalized = []\n",
    "for p in all_s2:\n",
    "    pid = p.get('paperId')\n",
    "    if pid and pid not in seen:\n",
    "        seen.add(pid)\n",
    "        authors = [a.get('name', '') for a in p.get('authors', [])]\n",
    "        s2_normalized.append({\n",
    "            'paper_id': pid,\n",
    "            'title': p.get('title', ''),\n",
    "            'authors': authors,\n",
    "            'abstract': p.get('abstract', ''),\n",
    "            'venue': p.get('venue', ''),\n",
    "            'year': p.get('year'),\n",
    "            'citation_count': p.get('citationCount', 0),\n",
    "            'categories': p.get('fieldsOfStudy', [])\n",
    "        })\n",
    "\n",
    "s2_df = pd.DataFrame(s2_normalized)\n",
    "s2_df = s2_df[(s2_df['title'].str.len() > 0) & (s2_df['abstract'].str.len() > 0)]\n",
    "print(f\"\\n‚úì S2ORC: {len(s2_df)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Normalization to Unified Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize to unified schema\n",
    "def normalize_to_schema(df, source):\n",
    "    \"\"\"Normalize dataframe to unified schema.\"\"\"\n",
    "    normalized = pd.DataFrame({\n",
    "        'paper_id': source + '_' + df['paper_id'].astype(str).str.replace('/', '_'),\n",
    "        'title': df['title'],\n",
    "        'authors': df['authors'],\n",
    "        'abstract': df['abstract'].fillna(''),\n",
    "        'venue': df['venue'].fillna(''),\n",
    "        'year': df['year'],\n",
    "        'categories': df.get('categories', [[]]*len(df)),\n",
    "        'source': source\n",
    "    })\n",
    "    return normalized\n",
    "\n",
    "print(\"\\nNormalizing datasets...\")\n",
    "arxiv_norm = normalize_to_schema(arxiv_df, 'arxiv')\n",
    "acl_norm = normalize_to_schema(acl_df, 'acl')\n",
    "s2_norm = normalize_to_schema(s2_df, 's2orc')\n",
    "\n",
    "# combine\n",
    "combined_df = pd.concat([arxiv_norm, acl_norm, s2_norm], ignore_index=True)\n",
    "print(f\"‚úì Combined: {len(combined_df)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dataset Validation\n",
    "\n",
    "**Comprehensive validation checks for data quality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET VALIDATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Missing abstracts\n",
    "missing_abstracts = combined_df['abstract'].str.len() == 0\n",
    "print(f\"\\n1. Missing Abstracts:\")\n",
    "print(f\"   Count: {missing_abstracts.sum()} papers\")\n",
    "print(f\"   Percentage: {missing_abstracts.sum()/len(combined_df)*100:.1f}%\")\n",
    "print(f\"   By source:\")\n",
    "for src in combined_df['source'].unique():\n",
    "    src_missing = combined_df[combined_df['source']==src]['abstract'].str.len() == 0\n",
    "    print(f\"     {src}: {src_missing.sum()} ({src_missing.sum()/len(combined_df[combined_df['source']==src])*100:.1f}%)\")\n",
    "\n",
    "# 2. Missing titles\n",
    "missing_titles = combined_df['title'].str.len() == 0\n",
    "print(f\"\\n2. Missing Titles: {missing_titles.sum()} papers\")\n",
    "\n",
    "# 3. Missing authors\n",
    "missing_authors = combined_df['authors'].apply(len) == 0\n",
    "print(f\"\\n3. Missing Authors:\")\n",
    "print(f\"   Count: {missing_authors.sum()} papers\")\n",
    "print(f\"   Percentage: {missing_authors.sum()/len(combined_df)*100:.1f}%\")\n",
    "\n",
    "# 4. Invalid years\n",
    "invalid_years = (combined_df['year'].isna()) | (combined_df['year'] < 1990) | (combined_df['year'] > 2025)\n",
    "print(f\"\\n4. Invalid Years: {invalid_years.sum()} papers\")\n",
    "if invalid_years.sum() > 0:\n",
    "    print(f\"   Year range: {combined_df[invalid_years]['year'].min()} - {combined_df[invalid_years]['year'].max()}\")\n",
    "\n",
    "# 5. Duplicate titles\n",
    "duplicates = combined_df.duplicated(subset=['title'], keep=False)\n",
    "print(f\"\\n5. Duplicate Titles:\")\n",
    "print(f\"   Total duplicate entries: {duplicates.sum()} papers\")\n",
    "print(f\"   Unique duplicated titles: {duplicates.sum()//2} titles\")\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"   Example duplicates:\")\n",
    "    dup_titles = combined_df[duplicates].groupby('title')['source'].apply(list).head(3)\n",
    "    for title, sources in dup_titles.items():\n",
    "        print(f\"     '{title[:60]}...' in sources: {sources}\")\n",
    "\n",
    "# 6. Short abstracts\n",
    "short_abstracts = (combined_df['abstract'].str.len() > 0) & (combined_df['abstract'].str.len() < 50)\n",
    "print(f\"\\n6. Short Abstracts (<50 chars): {short_abstracts.sum()} papers\")\n",
    "\n",
    "# 7. Missing venue\n",
    "missing_venue = combined_df['venue'].str.len() == 0\n",
    "print(f\"\\n7. Missing Venue: {missing_venue.sum()} papers ({missing_venue.sum()/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# 8. Missing categories\n",
    "missing_categories = combined_df['categories'].apply(lambda x: len(x) if isinstance(x, list) else 0) == 0\n",
    "print(f\"\\n8. Missing Categories: {missing_categories.sum()} papers ({missing_categories.sum()/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "total_issues = (missing_abstracts.sum() + missing_titles.sum() + \n",
    "                missing_authors.sum() + invalid_years.sum() + \n",
    "                duplicates.sum() + short_abstracts.sum())\n",
    "print(f\"Total papers with issues: {total_issues}\")\n",
    "print(f\"Clean papers: {len(combined_df) - total_issues}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Data Cleaning & Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaning dataset based on validation...\")\n",
    "\n",
    "# filter out invalid entries\n",
    "cleaned_df = combined_df[\n",
    "    (combined_df['title'].str.len() > 10) &\n",
    "    (combined_df['abstract'].str.len() >= 50) &\n",
    "    (combined_df['authors'].apply(len) > 0) &\n",
    "    (combined_df['year'] >= 1990) &\n",
    "    (combined_df['year'] <= 2025)\n",
    "].copy()\n",
    "\n",
    "print(f\"  After filtering: {len(cleaned_df)} papers\")\n",
    "print(f\"  Removed: {len(combined_df) - len(cleaned_df)} papers\")\n",
    "\n",
    "# deduplicate (priority: acl > s2orc > arxiv)\n",
    "source_priority = {'acl': 1, 's2orc': 2, 'arxiv': 3}\n",
    "cleaned_df['_priority'] = cleaned_df['source'].map(source_priority)\n",
    "cleaned_df = cleaned_df.sort_values('_priority').drop_duplicates(subset=['title'], keep='first')\n",
    "cleaned_df = cleaned_df.drop('_priority', axis=1)\n",
    "\n",
    "print(f\"  After deduplication: {len(cleaned_df)} papers\")\n",
    "print(f\"  Duplicates removed: {len(combined_df) - len(cleaned_df) - (len(combined_df) - len(cleaned_df))}\")\n",
    "\n",
    "# add computed fields\n",
    "cleaned_df['title_length'] = cleaned_df['title'].str.len()\n",
    "cleaned_df['abstract_length'] = cleaned_df['abstract'].str.len()\n",
    "cleaned_df['num_authors'] = cleaned_df['authors'].apply(len)\n",
    "\n",
    "print(\"\\n‚úì Data cleaning complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save complete dataset\n",
    "cleaned_df.to_parquet('data/raw/complete_dataset.parquet', index=False)\n",
    "\n",
    "print(f\"\\n‚úì Saved {len(cleaned_df)} papers to data/raw/complete_dataset.parquet\")\n",
    "print(f\"  File size: {os.path.getsize('data/raw/complete_dataset.parquet') / (1024*1024):.2f} MB\")\n",
    "\n",
    "print(\"\\nSource distribution:\")\n",
    "print(cleaned_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Full Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove URLs, emails, and extra whitespace.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"Remove special characters but keep basic punctuation.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?-]', '', text)\n",
    "    text = re.sub(r'([.,!?-])\\1+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect language of text.\"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "def segment_sentences(text):\n",
    "    \"\"\"Segment text into sentences.\"\"\"\n",
    "    try:\n",
    "        return sent_tokenize(text)\n",
    "    except:\n",
    "        return [text]\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords.\"\"\"\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatize tokens.\"\"\"\n",
    "    text = ' '.join(tokens)\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"‚úì Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_paper(text, lowercase=True, remove_stops=True, lemmatize=True):\n",
    "    \"\"\"Complete preprocessing pipeline with language detection and sentence segmentation.\"\"\"\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return {\n",
    "            'cleaned_text': '',\n",
    "            'language': 'unknown',\n",
    "            'sentences': [],\n",
    "            'num_sentences': 0,\n",
    "            'tokens': [],\n",
    "            'processed_text': ''\n",
    "        }\n",
    "    \n",
    "    # detect language\n",
    "    language = detect_language(text)\n",
    "    \n",
    "    # clean text\n",
    "    text = clean_text(text)\n",
    "    text = remove_special_chars(text)\n",
    "    \n",
    "    # sentence segmentation\n",
    "    sentences = segment_sentences(text)\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    cleaned_text = text\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stops:\n",
    "        tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    # filter short and non-alphanumeric tokens\n",
    "    tokens = [t for t in tokens if len(t) > 2 and t.isalnum()]\n",
    "    \n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return {\n",
    "        'cleaned_text': cleaned_text,\n",
    "        'language': language,\n",
    "        'sentences': sentences,\n",
    "        'num_sentences': len(sentences),\n",
    "        'tokens': tokens,\n",
    "        'processed_text': processed_text\n",
    "    }\n",
    "\n",
    "print(\"‚úì Complete preprocessing pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Apply Preprocessing to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load complete dataset\n",
    "df = pd.read_parquet('data/raw/complete_dataset.parquet')\n",
    "\n",
    "# sample for processing (or use full dataset)\n",
    "sample_size = 500  # change to len(df) for full dataset\n",
    "if len(df) > sample_size:\n",
    "    sample_df = df.sample(n=sample_size, random_state=42).copy()\n",
    "else:\n",
    "    sample_df = df.copy()\n",
    "\n",
    "print(f\"‚úì Processing {len(sample_df)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all papers\n",
    "print(f\"\\nPreprocessing {len(sample_df)} papers...\\n\")\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    result = preprocess_paper(row['abstract'])\n",
    "    \n",
    "    processed_data.append({\n",
    "        'paper_id': row['paper_id'],\n",
    "        'title': row['title'],\n",
    "        'authors': row['authors'],\n",
    "        'original_abstract': row['abstract'],\n",
    "        'cleaned_text': result['cleaned_text'],\n",
    "        'language': result['language'],\n",
    "        'sentences': result['sentences'],\n",
    "        'num_sentences': result['num_sentences'],\n",
    "        'tokens': result['tokens'],\n",
    "        'processed_text': result['processed_text'],\n",
    "        'num_tokens': len(result['tokens']),\n",
    "        'source': row['source'],\n",
    "        'year': row['year'],\n",
    "        'venue': row['venue']\n",
    "    })\n",
    "\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Preprocessing Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nToken Statistics:\")\n",
    "print(f\"  Average tokens per paper: {processed_df['num_tokens'].mean():.1f}\")\n",
    "print(f\"  Min tokens: {processed_df['num_tokens'].min()}\")\n",
    "print(f\"  Max tokens: {processed_df['num_tokens'].max()}\")\n",
    "print(f\"  Median tokens: {processed_df['num_tokens'].median():.1f}\")\n",
    "\n",
    "print(f\"\\nSentence Statistics:\")\n",
    "print(f\"  Average sentences per paper: {processed_df['num_sentences'].mean():.1f}\")\n",
    "print(f\"  Min sentences: {processed_df['num_sentences'].min()}\")\n",
    "print(f\"  Max sentences: {processed_df['num_sentences'].max()}\")\n",
    "\n",
    "print(f\"\\nLanguage Distribution:\")\n",
    "print(processed_df['language'].value_counts().head(10))\n",
    "\n",
    "# vocabulary\n",
    "all_tokens = []\n",
    "for tokens in processed_df['tokens']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "print(f\"\\nVocabulary:\")\n",
    "print(f\"  Vocabulary size: {len(token_counts):,}\")\n",
    "print(f\"  Total tokens: {len(all_tokens):,}\")\n",
    "\n",
    "print(f\"\\nTop 20 most common tokens:\")\n",
    "for token, count in token_counts.most_common(20):\n",
    "    print(f\"  {token}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned dataset\n",
    "processed_df.to_parquet('data/processed/cleaned_papers.parquet', index=False)\n",
    "\n",
    "# save vocabulary\n",
    "vocab_data = {\n",
    "    'vocab_size': len(token_counts),\n",
    "    'total_tokens': len(all_tokens),\n",
    "    'vocabulary': sorted(list(token_counts.keys())),\n",
    "    'token_frequencies': dict(token_counts.most_common(1000))\n",
    "}\n",
    "\n",
    "with open('data/processed/vocabulary.json', 'w') as f:\n",
    "    json.dump(vocab_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Saved cleaned dataset to data/processed/cleaned_papers.parquet\")\n",
    "print(f\"  File size: {os.path.getsize('data/processed/cleaned_papers.parquet') / (1024*1024):.2f} MB\")\n",
    "print(f\"‚úì Saved vocabulary to data/processed/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Embeddings Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepare Data for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned data\n",
    "embed_df = pd.read_parquet('data/processed/cleaned_papers.parquet')\n",
    "\n",
    "# prepare texts\n",
    "abstracts = embed_df['processed_text'].tolist()\n",
    "titles = embed_df['title'].tolist()\n",
    "paper_ids = embed_df['paper_id'].tolist()\n",
    "\n",
    "print(f\"‚úì Loaded {len(embed_df)} papers for embedding generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Word2Vec Embeddings (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Word2Vec model...\")\n",
    "\n",
    "# prepare tokenized sentences\n",
    "tokenized_abstracts = [text.split() for text in abstracts if text]\n",
    "\n",
    "# train Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_abstracts,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"‚úì Word2Vec trained\")\n",
    "print(f\"  Vocabulary size: {len(w2v_model.wv)}\")\n",
    "print(f\"  Vector size: {w2v_model.wv.vector_size}\")\n",
    "\n",
    "# generate document embeddings (average of word vectors)\n",
    "def get_w2v_embedding(text, model):\n",
    "    words = text.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "w2v_embeddings = np.array([get_w2v_embedding(text, w2v_model) for text in abstracts])\n",
    "\n",
    "print(f\"‚úì Generated Word2Vec embeddings: {w2v_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 SBERT Embeddings (Sentence-BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading SBERT model...\")\n",
    "\n",
    "# load pre-trained SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"‚úì SBERT model loaded\")\n",
    "print(f\"  Model: all-MiniLM-L6-v2\")\n",
    "print(f\"  Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SBERT embeddings for abstracts\n",
    "print(\"\\nGenerating SBERT embeddings for abstracts...\")\n",
    "sbert_abstract_embeddings = sbert_model.encode(\n",
    "    abstracts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì SBERT abstract embeddings: {sbert_abstract_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SBERT embeddings for titles\n",
    "print(\"\\nGenerating SBERT embeddings for titles...\")\n",
    "sbert_title_embeddings = sbert_model.encode(\n",
    "    titles,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì SBERT title embeddings: {sbert_title_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 SciBERT Embeddings (Scientific Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading SciBERT model...\")\n",
    "\n",
    "# load SciBERT model (optimized for scientific text)\n",
    "scibert_model = SentenceTransformer('allenai-specter')\n",
    "\n",
    "print(f\"‚úì SciBERT model loaded\")\n",
    "print(f\"  Model: allenai-specter\")\n",
    "print(f\"  Embedding dimension: {scibert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SciBERT embeddings\n",
    "print(\"\\nGenerating SciBERT embeddings...\")\n",
    "scibert_embeddings = scibert_model.encode(\n",
    "    abstracts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=16,  # smaller batch for larger model\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì SciBERT embeddings: {scibert_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Compare Embedding Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# compare first 10 papers\n",
    "sample_size = min(10, len(abstracts))\n",
    "\n",
    "print(f\"\\nComparing embeddings for first {sample_size} papers:\\n\")\n",
    "\n",
    "# Word2Vec\n",
    "w2v_sim = cosine_similarity(w2v_embeddings[:sample_size])\n",
    "print(f\"Word2Vec:\")\n",
    "print(f\"  Dimension: {w2v_embeddings.shape[1]}\")\n",
    "print(f\"  Avg pairwise similarity: {np.mean(w2v_sim[np.triu_indices_from(w2v_sim, k=1)]):.3f}\")\n",
    "\n",
    "# SBERT\n",
    "sbert_sim = cosine_similarity(sbert_abstract_embeddings[:sample_size])\n",
    "print(f\"\\nSBERT:\")\n",
    "print(f\"  Dimension: {sbert_abstract_embeddings.shape[1]}\")\n",
    "print(f\"  Avg pairwise similarity: {np.mean(sbert_sim[np.triu_indices_from(sbert_sim, k=1)]):.3f}\")\n",
    "\n",
    "# SciBERT\n",
    "scibert_sim = cosine_similarity(scibert_embeddings[:sample_size])\n",
    "print(f\"\\nSciBERT:\")\n",
    "print(f\"  Dimension: {scibert_embeddings.shape[1]}\")\n",
    "print(f\"  Avg pairwise similarity: {np.mean(scibert_sim[np.triu_indices_from(scibert_sim, k=1)]):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Use SBERT for general NLP tasks, SciBERT for scientific papers\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "print(\"\\nSaving embeddings...\")\n",
    "\n",
    "# Word2Vec\n",
    "np.save('data/embeddings/word2vec_embeddings.npy', w2v_embeddings)\n",
    "print(f\"‚úì Saved Word2Vec embeddings: {w2v_embeddings.shape}\")\n",
    "\n",
    "# SBERT abstract\n",
    "np.save('data/embeddings/sbert_abstract_embeddings.npy', sbert_abstract_embeddings)\n",
    "print(f\"‚úì Saved SBERT abstract embeddings: {sbert_abstract_embeddings.shape}\")\n",
    "\n",
    "# SBERT title\n",
    "np.save('data/embeddings/sbert_title_embeddings.npy', sbert_title_embeddings)\n",
    "print(f\"‚úì Saved SBERT title embeddings: {sbert_title_embeddings.shape}\")\n",
    "\n",
    "# SciBERT\n",
    "np.save('data/embeddings/scibert_embeddings.npy', scibert_embeddings)\n",
    "print(f\"‚úì Saved SciBERT embeddings: {scibert_embeddings.shape}\")\n",
    "\n",
    "# save paper index\n",
    "paper_index = {\n",
    "    'paper_ids': paper_ids,\n",
    "    'titles': titles,\n",
    "    'num_papers': len(paper_ids),\n",
    "    'embedding_methods': ['word2vec', 'sbert_abstract', 'sbert_title', 'scibert'],\n",
    "    'embedding_dims': {\n",
    "        'word2vec': w2v_embeddings.shape[1],\n",
    "        'sbert': sbert_abstract_embeddings.shape[1],\n",
    "        'scibert': scibert_embeddings.shape[1]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data/embeddings/paper_index.pkl', 'wb') as f:\n",
    "    pickle.dump(paper_index, f)\n",
    "\n",
    "print(f\"‚úì Saved paper index: {len(paper_ids)} papers\")\n",
    "\n",
    "# save metadata\n",
    "embedding_metadata = {\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'num_papers': len(paper_ids),\n",
    "    'embeddings': {\n",
    "        'word2vec': {\n",
    "            'file': 'word2vec_embeddings.npy',\n",
    "            'shape': list(w2v_embeddings.shape),\n",
    "            'model': 'Word2Vec (gensim)',\n",
    "            'params': {'vector_size': 100, 'window': 5}\n",
    "        },\n",
    "        'sbert_abstract': {\n",
    "            'file': 'sbert_abstract_embeddings.npy',\n",
    "            'shape': list(sbert_abstract_embeddings.shape),\n",
    "            'model': 'all-MiniLM-L6-v2'\n",
    "        },\n",
    "        'sbert_title': {\n",
    "            'file': 'sbert_title_embeddings.npy',\n",
    "            'shape': list(sbert_title_embeddings.shape),\n",
    "            'model': 'all-MiniLM-L6-v2'\n",
    "        },\n",
    "        'scibert': {\n",
    "            'file': 'scibert_embeddings.npy',\n",
    "            'shape': list(scibert_embeddings.shape),\n",
    "            'model': 'allenai-specter'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data/embeddings/embedding_metadata.json', 'w') as f:\n",
    "    json.dump(embedding_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved embedding metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DAY 1 DELIVERABLES - COMPLETE PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüì• PART 1: DATA INGESTION & NORMALIZATION\")\n",
    "print(f\"  ‚úì ArXiv papers ingested: {len(arxiv_df):,}\")\n",
    "print(f\"  ‚úì ACL papers ingested: {len(acl_df):,}\")\n",
    "print(f\"  ‚úì S2ORC papers ingested: {len(s2_df):,}\")\n",
    "print(f\"  ‚úì Total combined: {len(combined_df):,}\")\n",
    "print(f\"  ‚úì After validation & cleaning: {len(cleaned_df):,}\")\n",
    "print(f\"  ‚úì Output: data/raw/complete_dataset.parquet\")\n",
    "\n",
    "print(\"\\nüßπ PART 2: FULL PREPROCESSING PIPELINE\")\n",
    "print(f\"  ‚úì Papers preprocessed: {len(processed_df):,}\")\n",
    "print(f\"  ‚úì Language detection: ‚úì\")\n",
    "print(f\"  ‚úì Sentence segmentation: ‚úì\")\n",
    "print(f\"  ‚úì Avg tokens per paper: {processed_df['num_tokens'].mean():.1f}\")\n",
    "print(f\"  ‚úì Vocabulary size: {len(token_counts):,}\")\n",
    "print(f\"  ‚úì Output: data/processed/cleaned_papers.parquet\")\n",
    "\n",
    "print(\"\\nüî¢ PART 3: EMBEDDINGS PIPELINE\")\n",
    "print(f\"  ‚úì Word2Vec embeddings: {w2v_embeddings.shape}\")\n",
    "print(f\"  ‚úì SBERT abstract embeddings: {sbert_abstract_embeddings.shape}\")\n",
    "print(f\"  ‚úì SBERT title embeddings: {sbert_title_embeddings.shape}\")\n",
    "print(f\"  ‚úì SciBERT embeddings: {scibert_embeddings.shape}\")\n",
    "print(f\"  ‚úì Output: data/embeddings/*.npy, paper_index.pkl\")\n",
    "\n",
    "print(\"\\nüìÅ OUTPUT FILES\")\n",
    "output_files = [\n",
    "    'data/raw/complete_dataset.parquet',\n",
    "    'data/processed/cleaned_papers.parquet',\n",
    "    'data/processed/vocabulary.json',\n",
    "    'data/embeddings/word2vec_embeddings.npy',\n",
    "    'data/embeddings/sbert_abstract_embeddings.npy',\n",
    "    'data/embeddings/sbert_title_embeddings.npy',\n",
    "    'data/embeddings/scibert_embeddings.npy',\n",
    "    'data/embeddings/paper_index.pkl',\n",
    "    'data/embeddings/embedding_metadata.json'\n",
    "]\n",
    "\n",
    "total_size = 0\n",
    "for filepath in output_files:\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"  ‚úì {filepath} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\n  Total storage: {total_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DAY 1 DELIVERABLES COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Next Steps (Day 2):\")\n",
    "print(\"  1. Train classification models\")\n",
    "print(\"  2. Build topic models (LDA)\")\n",
    "print(\"  3. Implement retrieval system (BM25 + semantic)\")\n",
    "print(\"  4. Create research digest interface\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

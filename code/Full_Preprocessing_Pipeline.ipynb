{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSJHsizxfHpi",
        "outputId": "18ee83f7-250a-418f-83d7-ceae75d8be03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Final-Project-Group-LexiCore'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (277/277), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 277 (delta 109), reused 239 (delta 95), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (277/277), 23.47 MiB | 22.42 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "/content/Final-Project-Group-LexiCore\n",
            " code\t\t\t    'Final Group Project Report'   README.md\n",
            "'Final Group Presentation'   LICENSE\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content\n",
        "\n",
        "\n",
        "!git clone https://github.com/adikan2k/Final-Project-Group-LexiCore\n",
        "\n",
        "\n",
        "%cd Final-Project-Group-LexiCore\n",
        "\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpK9TJJ-n5pM",
        "outputId": "421a415d-e138-4945-e031-e9f8c27784bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install spacy langdetect pyarrow pandas nltk\n",
        "\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhl6vfqmoEYD",
        "outputId": "d2be1d2f-71bd-4c52-cb31-ade8610858fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.py  preprocess.py  __pycache__\n"
          ]
        }
      ],
      "source": [
        "!ls src/preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdt9nQbaoRH7"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from langdetect import detect, LangDetectException\n",
        "import unicodedata\n",
        "import time\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
        "nlp.max_length = 2_000_000  \n",
        "\n",
        "def get_raw_text(row, full_col=\"full_text\", abstract_col=\"abstract\"):\n",
        "    \"\"\"\n",
        "    Prefer full text if available, otherwise abstract.\n",
        "    Change column names here if your dataset uses different ones.\n",
        "    \"\"\"\n",
        "    full = row.get(full_col)\n",
        "    abstract = row.get(abstract_col)\n",
        "    if isinstance(full, str) and full.strip():\n",
        "        return full\n",
        "    if isinstance(abstract, str) and abstract.strip():\n",
        "        return abstract\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def normalize_unicode(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "\n",
        "def is_english(text: str, min_chars: int = 200) -> bool:\n",
        "    \"\"\"\n",
        "    Detect if text is English using langdetect on a small sample.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return False\n",
        "    sample = text[: max(min_chars, 50)]\n",
        "    try:\n",
        "        return detect(sample) == \"en\"\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "\n",
        "def get_sentences(text: str):\n",
        "    \"\"\"\n",
        "    Split text into sentences using spaCy, for summarization later.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    doc = nlp(text)\n",
        "    return [s.text.strip() for s in doc.sents if s.text.strip()]\n",
        "\n",
        "\n",
        "def run_full_preprocessing_pipeline(\n",
        "    df: pd.DataFrame,\n",
        "    full_col: str = \"full_text\",\n",
        "    abstract_col: str = \"abstract\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Uses your teammate's preprocess_text() plus extra steps:\n",
        "\n",
        "    - build raw_text from full + abstract\n",
        "    - unicode normalization\n",
        "    - regex cleaning (clean_text + remove_special_chars)\n",
        "    - language detection (drop non-English)\n",
        "    - sentence splitting\n",
        "    - tokenization/lemmatization via preprocess_text\n",
        "    - num_tokens (benchmark)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        " \n",
        "    print(\"Step 1: create raw_text from full + abstract...\")\n",
        "    df[\"raw_text\"] = df.apply(\n",
        "        lambda row: get_raw_text(row, full_col=full_col, abstract_col=abstract_col),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    print(\"Step 2: unicode normalization + regex cleaning...\")\n",
        "    df[\"raw_text_clean\"] = (\n",
        "        df[\"raw_text\"]\n",
        "        .astype(str)\n",
        "        .apply(normalize_unicode)\n",
        "        .apply(clean_text)          \n",
        "        .apply(remove_special_chars)  \n",
        "    )\n",
        "\n",
        "   \n",
        "    print(\"Step 3: language detection (English only)...\")\n",
        "    df[\"is_english\"] = df[\"raw_text_clean\"].apply(is_english)\n",
        "    before = len(df)\n",
        "    df = df[df[\"is_english\"]].reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    print(f\" Kept {after} / {before} papers as English\")\n",
        "\n",
        "    \n",
        "    print(\"Step 4: sentence splitting + token/lemma processing...\")\n",
        "    sentences_list = []\n",
        "    processed_texts = []\n",
        "    num_tokens = []\n",
        "\n",
        "    for text in df[\"raw_text_clean\"]:\n",
        "        sents = get_sentences(text)\n",
        "        result = preprocess_text(text)  \n",
        "        processed_texts.append(result[\"processed_text\"])\n",
        "        num_tokens.append(len(result[\"tokens\"]))\n",
        "        sentences_list.append(sents)\n",
        "\n",
        "    df[\"sentences\"] = sentences_list      \n",
        "    df[\"text_clean\"] = processed_texts    \n",
        "    df[\"num_tokens\"] = num_tokens         \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHmU00bBoZgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUwXe8zAoKKF",
        "outputId": "de0d5700-3bab-4f06-e26c-c5ff0d5d8b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imported preprocess_text, clean_text, remove_special_chars \n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.abspath(\".\")) \n",
        "\n",
        "from src.preprocessing.preprocess import preprocess_text, clean_text, remove_special_chars\n",
        "\n",
        "print(\"Imported preprocess_text, clean_text, remove_special_chars \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKkqvM0boznh",
        "outputId": "94f84f3a-4a91-449f-b6f6-6ca255f21985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading: data/raw/complete_dataset.parquet\n",
            "Rows loaded: 1101\n",
            "\n",
            "Columns:\n",
            " ['paper_id', 'title', 'authors', 'abstract', 'venue', 'year', 'categories', 'source', 'title_length', 'abstract_length', 'num_authors']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "RAW = Path(\"data/raw/complete_dataset.parquet\")\n",
        "print(\"Loading:\", RAW)\n",
        "\n",
        "df_raw = pd.read_parquet(RAW)\n",
        "print(\"Rows loaded:\", len(df_raw))\n",
        "print(\"\\nColumns:\\n\", df_raw.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hSU0vKqogJh"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from langdetect import detect, LangDetectException\n",
        "import unicodedata\n",
        "import time\n",
        "\n",
        "# spaCy just for sentence splitting\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
        "nlp.max_length = 2_000_000  # handle long S2ORC docs\n",
        "\n",
        "\n",
        "def get_raw_text(row, full_col=\"full_text\", abstract_col=\"abstract\"):\n",
        "    \"\"\"\n",
        "    Prefer full text if available, otherwise abstract.\n",
        "    Change column names here if your dataset uses different ones.\n",
        "    \"\"\"\n",
        "    full = row.get(full_col)\n",
        "    abstract = row.get(abstract_col)\n",
        "    if isinstance(full, str) and full.strip():\n",
        "        return full\n",
        "    if isinstance(abstract, str) and abstract.strip():\n",
        "        return abstract\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def normalize_unicode(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "\n",
        "def is_english(text: str, min_chars: int = 200) -> bool:\n",
        "    \"\"\"\n",
        "    Detect if text is English using langdetect on a small sample.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return False\n",
        "    sample = text[: max(min_chars, 50)]\n",
        "    try:\n",
        "        return detect(sample) == \"en\"\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "\n",
        "def get_sentences(text: str):\n",
        "    \"\"\"\n",
        "    Split text into sentences using spaCy, for summarization later.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    doc = nlp(text)\n",
        "    return [s.text.strip() for s in doc.sents if s.text.strip()]\n",
        "\n",
        "\n",
        "def run_full_preprocessing_pipeline(\n",
        "    df: pd.DataFrame,\n",
        "    full_col: str = \"full_text\",\n",
        "    abstract_col: str = \"abstract\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Uses your teammate's preprocess_text() plus extra steps:\n",
        "\n",
        "    - build raw_text from full + abstract\n",
        "    - unicode normalization\n",
        "    - regex cleaning (clean_text + remove_special_chars)\n",
        "    - language detection (drop non-English)\n",
        "    - sentence splitting\n",
        "    - tokenization/lemmatization via preprocess_text\n",
        "    - num_tokens (benchmark)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    \n",
        "    print(\"Step 1: create raw_text from full + abstract...\")\n",
        "    df[\"raw_text\"] = df.apply(\n",
        "        lambda row: get_raw_text(row, full_col=full_col, abstract_col=abstract_col),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "   \n",
        "    print(\"Step 2: unicode normalization + regex cleaning...\")\n",
        "    df[\"raw_text_clean\"] = (\n",
        "        df[\"raw_text\"]\n",
        "        .astype(str)\n",
        "        .apply(normalize_unicode)\n",
        "        .apply(clean_text)         \n",
        "        .apply(remove_special_chars)  \n",
        "    )\n",
        "\n",
        "    print(\"Step 3: language detection (English only)...\")\n",
        "    df[\"is_english\"] = df[\"raw_text_clean\"].apply(is_english)\n",
        "    before = len(df)\n",
        "    df = df[df[\"is_english\"]].reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    print(f\" Kept {after} / {before} papers as English\")\n",
        "\n",
        "    print(\"Step 4: sentence splitting + token/lemma processing...\")\n",
        "    sentences_list = []\n",
        "    processed_texts = []\n",
        "    num_tokens = []\n",
        "\n",
        "    for text in df[\"raw_text_clean\"]:\n",
        "        sents = get_sentences(text)\n",
        "        result = preprocess_text(text) \n",
        "        processed_texts.append(result[\"processed_text\"])\n",
        "        num_tokens.append(len(result[\"tokens\"]))\n",
        "        sentences_list.append(sents)\n",
        "\n",
        "    df[\"sentences\"] = sentences_list      \n",
        "    df[\"text_clean\"] = processed_texts   \n",
        "    df[\"num_tokens\"] = num_tokens        \n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEygVqZ_o7uL",
        "outputId": "233ec589-e96f-4207-85bc-84603c3a643b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: create raw_text from full + abstract...\n",
            "Step 2: unicode normalization + regex cleaning...\n",
            "Step 3: language detection (English only)...\n",
            " Kept 1101 / 1101 papers as English\n",
            "Step 4: sentence splitting + token/lemma processing...\n"
          ]
        }
      ],
      "source": [
        "df_clean = run_full_preprocessing_pipeline(df_raw,\n",
        "                                           full_col=\"body_text\",\n",
        "                                           abstract_col=\"abstract\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHKQHYhhpCjd",
        "outputId": "1ec0344d-69e8-448a-9919-4b9f096970d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: create raw_text from full + abstract...\n",
            "Step 2: unicode normalization + regex cleaning...\n",
            "Step 3: language detection (English only)...\n",
            " Kept 1101 / 1101 papers as English\n",
            "Step 4: sentence splitting + token/lemma processing...\n",
            "\n",
            "‚è± Total time: 123.1 seconds\n",
            "‚è± Avg per paper: 0.1118 seconds\n",
            "\n",
            "üìä Tokens per paper (num_tokens):\n",
            "count    1101.000000\n",
            "mean      150.742961\n",
            "std        41.176717\n",
            "min        20.000000\n",
            "25%       124.000000\n",
            "50%       150.000000\n",
            "75%       177.000000\n",
            "max       466.000000\n",
            "Name: num_tokens, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "FULL_COL = \"full_text\"\n",
        "ABSTRACT_COL = \"abstract\"\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "df_clean = run_full_preprocessing_pipeline(\n",
        "    df_raw,\n",
        "    full_col=FULL_COL,\n",
        "    abstract_col=ABSTRACT_COL,\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(\"\\n‚è± Total time:\", round(elapsed, 2), \"seconds\")\n",
        "print(\"‚è± Avg per paper:\", round(elapsed / max(len(df_clean), 1), 4), \"seconds\")\n",
        "\n",
        "print(\"\\nüìä Tokens per paper (num_tokens):\")\n",
        "print(df_clean[\"num_tokens\"].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdV7ZWvvp-3V",
        "outputId": "fcd8e5f2-4741-4452-943b-6b715a114e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Saved cleaned data to: data/preprocessed/cleaned_papers.parquet\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = Path(\"data/preprocessed\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT = OUT_DIR / \"cleaned_papers.parquet\"\n",
        "df_clean.to_parquet(OUT, index=False)\n",
        "\n",
        "print(\"üíæ Saved cleaned data to:\", OUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BRDrMbYSsY5v"
      },
      "outputs": [],
      "source": [
        "!find . -name \"*.ipynb\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

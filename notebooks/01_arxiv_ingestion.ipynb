{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Data Ingestion\n",
    "## Goal: Download and parse ArXiv bulk data for NLP papers\n",
    "\n",
    "We're focusing on cs.CL (Computation and Language), cs.LG (Machine Learning), and stat.ML categories.\n",
    "\n",
    "**Note:** ArXiv bulk data is HUGE. For testing, we'll work with a subset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install pandas arxiv pyarrow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import arxiv\n",
    "from google.colab import drive\n",
    "\n",
    "# mount drive to save data\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories if they don't exist\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "print(\"Directories created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using arxiv API (easier, but limited to 2000 papers)\n",
    "\n",
    "Let's start with this approach for prototyping. We can switch to bulk data later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the API first with a small query\n",
    "client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query = 'cat:cs.CL',\n",
    "    max_results = 5,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "# let's see what we get\n",
    "for result in client.results(search):\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"Authors: {', '.join([a.name for a in result.authors])}\")\n",
    "    print(f\"Published: {result.published}\")\n",
    "    print(f\"Categories: {result.categories}\")\n",
    "    print(\"Abstract:\", result.summary[:200], \"...\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now let's write a function to fetch papers from multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_papers(categories, max_results_per_category=500):\n",
    "    \"\"\"\n",
    "    Fetch papers from arxiv for given categories\n",
    "    \n",
    "    Args:\n",
    "        categories: list of category strings like ['cs.CL', 'cs.LG']\n",
    "        max_results_per_category: how many papers to fetch per category\n",
    "    \n",
    "    Returns:\n",
    "        list of paper dictionaries\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(f\"Fetching papers from {cat}...\")\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query = f'cat:{cat}',\n",
    "            max_results = max_results_per_category,\n",
    "            sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "        \n",
    "        count = 0\n",
    "        for result in client.results(search):\n",
    "            paper = {\n",
    "                'paper_id': result.entry_id.split('/')[-1],  # extract ID from URL\n",
    "                'title': result.title,\n",
    "                'authors': [author.name for author in result.authors],\n",
    "                'abstract': result.summary.replace('\\n', ' '),  # clean up newlines\n",
    "                'categories': result.categories,\n",
    "                'primary_category': result.primary_category,\n",
    "                'published': result.published.isoformat(),\n",
    "                'updated': result.updated.isoformat(),\n",
    "                'pdf_url': result.pdf_url,\n",
    "                'venue': 'arXiv',\n",
    "                'year': result.published.year\n",
    "            }\n",
    "            papers.append(paper)\n",
    "            count += 1\n",
    "            \n",
    "            # print progress every 50 papers\n",
    "            if count % 50 == 0:\n",
    "                print(f\"  Fetched {count} papers from {cat}...\")\n",
    "        \n",
    "        print(f\"Completed {cat}: {count} papers fetched\\n\")\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our target categories\n",
    "TARGET_CATEGORIES = ['cs.CL', 'cs.LG', 'stat.ML']\n",
    "\n",
    "# start with smaller number for testing, increase later\n",
    "# can go up to 2000 or so with API\n",
    "papers_data = fetch_arxiv_papers(TARGET_CATEGORIES, max_results_per_category=300)\n",
    "\n",
    "print(f\"\\nTotal papers fetched: {len(papers_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check what we got\n",
    "print(f\"Sample paper structure:\")\n",
    "print(json.dumps(papers_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to JSON and Parquet\n",
    "\n",
    "We'll save in both formats:\n",
    "- JSON for easy inspection\n",
    "- Parquet for efficient storage and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as JSON first\n",
    "json_path = 'data/raw/arxiv_papers.json'\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(papers_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(papers_data)} papers to {json_path}\")\n",
    "\n",
    "# check file size\n",
    "file_size = os.path.getsize(json_path) / (1024 * 1024)  # convert to MB\n",
    "print(f\"File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe for parquet\n",
    "df = pd.DataFrame(papers_data)\n",
    "\n",
    "# check the dataframe\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some stats\n",
    "print(\"\\nPapers per category:\")\n",
    "print(df['primary_category'].value_counts())\n",
    "\n",
    "print(\"\\nPapers per year:\")\n",
    "print(df['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parquet\n",
    "parquet_path = 'data/raw/arxiv_papers.parquet'\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"Saved to {parquet_path}\")\n",
    "\n",
    "# check parquet file size\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024 * 1024)\n",
    "print(f\"Parquet file size: {parquet_size:.2f} MB\")\n",
    "print(f\"Compression ratio: {file_size/parquet_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nChecking abstracts...\")\n",
    "print(f\"Min abstract length: {df['abstract'].str.len().min()}\")\n",
    "print(f\"Max abstract length: {df['abstract'].str.len().max()}\")\n",
    "print(f\"Mean abstract length: {df['abstract'].str.len().mean():.2f}\")\n",
    "\n",
    "# find papers with very short abstracts (potential issues)\n",
    "short_abstracts = df[df['abstract'].str.len() < 100]\n",
    "print(f\"\\nPapers with abstracts < 100 chars: {len(short_abstracts)}\")\n",
    "if len(short_abstracts) > 0:\n",
    "    print(\"Example:\")\n",
    "    print(short_abstracts[['title', 'abstract']].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates based on paper_id\n",
    "duplicates = df[df.duplicated(subset=['paper_id'], keep=False)]\n",
    "print(f\"Duplicate paper IDs found: {len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates(subset=['paper_id'], keep='first')\n",
    "    print(f\"New shape: {df.shape}\")\n",
    "    \n",
    "    # re-save the cleaned data\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    print(\"Updated parquet file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few sample papers\n",
    "print(\"Sample papers:\")\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['title']}\")\n",
    "    print(f\"   Authors: {', '.join(row['authors'][:3])}{'...' if len(row['authors']) > 3 else ''}\")\n",
    "    print(f\"   Year: {row['year']} | Category: {row['primary_category']}\")\n",
    "    print(f\"   Abstract: {row['abstract'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Copy to Google Drive for persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save to drive\n",
    "# !cp data/raw/arxiv_papers.parquet /content/drive/MyDrive/\n",
    "# !cp data/raw/arxiv_papers.json /content/drive/MyDrive/\n",
    "# print(\"Files copied to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ Fetched papers from ArXiv API  \n",
    "✅ Extracted metadata (title, authors, abstract, categories, dates)  \n",
    "✅ Saved to JSON and Parquet formats  \n",
    "✅ Performed basic quality checks  \n",
    "\n",
    "**Next steps:**\n",
    "- Fetch more papers if needed (increase max_results)\n",
    "- Consider bulk data download for comprehensive coverage\n",
    "- Move to ACL Anthology ingestion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

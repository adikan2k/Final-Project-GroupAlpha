{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scholarly Topic Navigator - Complete Pipeline\n",
    "\n",
    "## ğŸ“š Project Overview\n",
    "\n",
    "This project builds an automated system to help faculty labs navigate the rapidly expanding landscape of NLP publications. We process papers from ArXiv, ACL Anthology, and S2ORC to create an explainable research digest pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—‚ï¸ Notebook Workflow\n",
    "\n",
    "Run these notebooks **in order** for the complete pipeline:\n",
    "\n",
    "### Phase 1: Data Ingestion\n",
    "1. **`01_arxiv_ingestion.ipynb`** - Fetch papers from ArXiv API\n",
    "   - Categories: cs.CL, cs.LG, stat.ML\n",
    "   - Output: `data/raw/arxiv_papers.parquet`\n",
    "   - Runtime: ~30-45 min\n",
    "\n",
    "2. **`02_acl_anthology_ingestion.ipynb`** - Download ACL papers\n",
    "   - Source: ACL Anthology BibTeX dump\n",
    "   - Output: `data/raw/acl_anthology_papers.parquet`\n",
    "   - Runtime: ~60-90 min\n",
    "\n",
    "3. **`03_s2orc_ingestion.ipynb`** - Fetch from Semantic Scholar\n",
    "   - Source: Semantic Scholar API\n",
    "   - Output: `data/raw/s2orc_papers.parquet`\n",
    "   - Runtime: ~20-30 min\n",
    "\n",
    "4. **`04_unified_metadata.ipynb`** - Combine all sources\n",
    "   - Merges and deduplicates data\n",
    "   - Output: `data/processed/unified_papers.parquet`\n",
    "   - Runtime: ~15-20 min\n",
    "\n",
    "### Phase 2: Preprocessing\n",
    "5. **`05_preprocessing_pipeline.ipynb`** - Text preprocessing\n",
    "   - Cleaning, tokenization, lemmatization\n",
    "   - Output: `data/processed/preprocessed_sample_500.parquet`\n",
    "   - Runtime: ~10-15 min\n",
    "\n",
    "### Phase 3: Embeddings (Coming Soon)\n",
    "6. **`06_embeddings_generation.ipynb`** - Generate embeddings\n",
    "   - Word2Vec, BERT, RoBERTa\n",
    "   - Output: `data/embeddings/`\n",
    "\n",
    "### Phase 4: Classification (Coming Soon)\n",
    "7. **`07_classification.ipynb`** - Topic classification\n",
    "   - Train classifiers\n",
    "   - Zero-shot classification\n",
    "\n",
    "### Phase 5: Topic Modeling (Coming Soon)\n",
    "8. **`08_topic_modeling.ipynb`** - Discover topics\n",
    "   - LDA, coherence analysis\n",
    "\n",
    "### Phase 6: Retrieval (Coming Soon)\n",
    "9. **`09_retrieval_system.ipynb`** - BM25 + Semantic search\n",
    "\n",
    "### Phase 7: Final Pipeline (Coming Soon)\n",
    "10. **`10_complete_pipeline_demo.ipynb`** - End-to-end demo\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Current Progress\n",
    "\n",
    "- [x] Data Ingestion (Notebooks 01-04)\n",
    "- [x] Preprocessing (Notebook 05)\n",
    "- [ ] Embeddings (Notebook 06)\n",
    "- [ ] Classification (Notebook 07)\n",
    "- [ ] Topic Modeling (Notebook 08)\n",
    "- [ ] Retrieval (Notebook 09)\n",
    "- [ ] Complete Pipeline (Notebook 10)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Expected Data Flow\n",
    "\n",
    "```\n",
    "Raw Papers (ArXiv, ACL, S2ORC)\n",
    "    â†“\n",
    "Unified Dataset (~42K-63K papers)\n",
    "    â†“\n",
    "Preprocessed Text (cleaned, tokenized, lemmatized)\n",
    "    â†“\n",
    "Embeddings (Word2Vec, BERT)\n",
    "    â†“\n",
    "Classification + Topic Models\n",
    "    â†“\n",
    "Retrieval System (BM25 + Semantic)\n",
    "    â†“\n",
    "Research Digest Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "\n",
    "### Option 1: Run All (Sequential)\n",
    "If you have time, run all notebooks in order to build the complete pipeline.\n",
    "\n",
    "### Option 2: Start from Preprocessed Data\n",
    "If you already have the unified dataset:\n",
    "1. Skip to notebook 05 (preprocessing)\n",
    "2. Continue with embeddings and classification\n",
    "\n",
    "### Option 3: Use Sample Data\n",
    "Each notebook includes code to generate sample data for testing.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Data Directory Structure\n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ raw/                          # Raw ingested data\n",
    "â”‚   â”œâ”€â”€ arxiv_papers.parquet\n",
    "â”‚   â”œâ”€â”€ acl_anthology_papers.parquet\n",
    "â”‚   â””â”€â”€ s2orc_papers.parquet\n",
    "â”‚\n",
    "â”œâ”€â”€ processed/                    # Cleaned and unified data\n",
    "â”‚   â”œâ”€â”€ unified_papers.parquet\n",
    "â”‚   â”œâ”€â”€ preprocessed_sample_500.parquet\n",
    "â”‚   â””â”€â”€ vocabulary.json\n",
    "â”‚\n",
    "â””â”€â”€ embeddings/                   # Generated embeddings\n",
    "    â”œâ”€â”€ word2vec/\n",
    "    â””â”€â”€ bert/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Setup Requirements\n",
    "\n",
    "### For Google Colab (Recommended)\n",
    "All packages are installed automatically in each notebook.\n",
    "\n",
    "### For Local Setup\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Documentation\n",
    "\n",
    "- **README.md** - Project overview\n",
    "- **DAY1_SETUP.md** - Data ingestion guide\n",
    "- **PREPROCESSING_GUIDE.md** - Preprocessing details\n",
    "- **QUICK_START.md** - Quick start instructions\n",
    "- **notebooks/README.md** - Notebook documentation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘¥ Team\n",
    "\n",
    "- **Aditya Kanbargi** - Data Engineering & Ingestion\n",
    "- **Trisha Singh** - Preprocessing & Embeddings\n",
    "- **Pramod Krishnachari** - Classification & Topic Modeling\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Next Steps\n",
    "\n",
    "1. **If starting fresh:** Begin with notebook 01\n",
    "2. **If continuing:** Check which notebook you completed last\n",
    "3. **If testing:** Jump to any notebook and use sample data\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Important Notes\n",
    "\n",
    "- **Runtime:** Complete pipeline takes 3-4 hours\n",
    "- **Storage:** Requires ~500MB-1GB disk space\n",
    "- **Internet:** Required for data ingestion notebooks\n",
    "- **Order:** Notebooks must be run sequentially\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to start? Open `01_arxiv_ingestion.ipynb` â†’**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/adikan2k/Final-Project-Group-LexiCore.git\n",
        "\n",
        "%cd /content/Final-Project-Group-LexiCore\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGC7ArYH3DFG",
        "outputId": "5cd534be-1d6a-4a28-c897-c144c2a80340"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Final-Project-Group-LexiCore'...\n",
            "remote: Enumerating objects: 450, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 450 (delta 31), reused 25 (delta 13), pack-reused 384 (from 2)\u001b[K\n",
            "Receiving objects: 100% (450/450), 319.90 MiB | 27.75 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n",
            "/content/Final-Project-Group-LexiCore\n",
            " code\t\t\t    'Final Group Project Report'    Outputs\n",
            " data\t\t\t     Final-Project-Group-LexiCore   README.md\n",
            "'Final Group Presentation'   LICENSE\t\t\t    run_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_pipeline.py\n",
        "\"\"\"\n",
        "Final Pipeline Orchestrator\n",
        "\n",
        "This script:\n",
        "  1. Loads the cleaned corpus (output of Day 1+2).\n",
        "  2. Runs retrieval over title + abstract.\n",
        "  3. Calls the summarization engine (generate_summaries).\n",
        "  4. Saves a final digest JSON.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "ROOT = Path(__file__).resolve().parent\n",
        "\n",
        "\n",
        "\n",
        "ROOT = Path(__file__).resolve().parent\n",
        "CODE_DIR = ROOT / \"code\"\n",
        "sys.path.append(str(CODE_DIR))\n",
        "\n",
        "try:\n",
        "\n",
        "    from summarization import generate_summaries  # type: ignore\n",
        "except Exception:\n",
        "\n",
        "    def generate_summaries(text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Fallback summary used only if real summarization.py is not available.\"\"\"\n",
        "        snippet = (text or \"\").strip()\n",
        "        one_sentence = snippet[:250]\n",
        "        return {\n",
        "            \"one_sentence\": one_sentence,\n",
        "            \"three_sentence\": one_sentence,\n",
        "            \"five_bullets\": [\n",
        "                f\"- {one_sentence}\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "def load_corpus() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load cleaned paper corpus produced by Day 1/2.\n",
        "\n",
        "    Tries both:\n",
        "      data/processed/cleaned_paper.parquet\n",
        "      data/processed/cleaned_papers.parquet\n",
        "    because naming sometimes differs.\n",
        "    \"\"\"\n",
        "    base = ROOT / \"data\" / \"processed\"\n",
        "    candidates = [\n",
        "        base / \"cleaned_paper.parquet\",\n",
        "        base / \"cleaned_papers.parquet\",\n",
        "    ]\n",
        "    last_err = None\n",
        "    for path in candidates:\n",
        "        try:\n",
        "            print(f\"Trying to load: {path}\")\n",
        "            df = pd.read_parquet(path)\n",
        "            print(f\"Loaded {len(df)} rows from {path.name}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not load cleaned parquet. Tried {', '.join(str(c) for c in candidates)}.\\n\"\n",
        "        f\"Last error: {last_err}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def simple_retrieval(query: str, df: pd.DataFrame, top_k: int = 5) -> List[int]:\n",
        "    \"\"\"\n",
        "    Very simple keyword retrieval over title + abstract.\n",
        "\n",
        "    (If you later want to plug in BM25/FAISS hybrid, replace this\n",
        "     with calls to your Day-2 retrieval engine.)\n",
        "    \"\"\"\n",
        "    text_series = (\n",
        "        df.get(\"title\", \"\").fillna(\"\").astype(str)\n",
        "        + \" \"\n",
        "        + df.get(\"original_abstract\", df.get(\"abstract\", \"\")).fillna(\"\").astype(str)\n",
        "    )\n",
        "\n",
        "    mask = text_series.str.contains(query, case=False, na=False)\n",
        "    indices = df[mask].index.tolist()\n",
        "\n",
        "    if not indices:\n",
        "        print(\"No exact keyword matches found; falling back to first top_k papers.\")\n",
        "        indices = df.index.tolist()\n",
        "\n",
        "    return indices[:top_k]\n",
        "\n",
        "\n",
        "def build_digest(query: str, df: pd.DataFrame, top_k: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Build digest object:\n",
        "      - retrieval to get top_k docs\n",
        "      - summarization for each abstract\n",
        "    \"\"\"\n",
        "    indices = simple_retrieval(query, df, top_k)\n",
        "    papers_out = []\n",
        "\n",
        "    for rank, idx in enumerate(indices, start=1):\n",
        "        row = df.loc[idx]\n",
        "        abstract = row.get(\"original_abstract\", row.get(\"abstract\", \"\"))\n",
        "\n",
        "        summaries = generate_summaries(str(abstract))\n",
        "\n",
        "        papers_out.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"paper_id\": row.get(\"paper_id\", int(idx)),\n",
        "                \"title\": row.get(\"title\", \"Untitled\"),\n",
        "                \"venue\": row.get(\"venue\"),\n",
        "                \"year\": int(row[\"year\"]) if \"year\" in row and pd.notna(row[\"year\"]) else None,\n",
        "                \"summaries\": summaries,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"num_results\": len(papers_out),\n",
        "        \"papers\": papers_out,\n",
        "    }\n",
        "\n",
        "\n",
        "def save_digest(digest: Dict[str, Any], output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Save digest to JSON, converting numpy/pandas types to plain Python types.\n",
        "    \"\"\"\n",
        "    folder = os.path.dirname(output_path)\n",
        "    if folder:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    def convert(o):\n",
        "        if isinstance(o, (np.integer,)):\n",
        "            return int(o)\n",
        "        if isinstance(o, (np.floating,)):\n",
        "            return float(o)\n",
        "        if isinstance(o, np.ndarray):\n",
        "            return o.tolist()\n",
        "        return str(o)\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(digest, f, indent=2, default=convert)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    query: str,\n",
        "    top_k: int = 5,\n",
        "    output_path: str = \"Outputs/pipeline_digest.json\",\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline:\n",
        "      1. Load corpus\n",
        "      2. Retrieve top_k docs\n",
        "      3. Summarize each\n",
        "      4. Save digest JSON\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    print(f\" Running pipeline for query: '{query}'\")\n",
        "    print(f\"   top_k       = {top_k}\")\n",
        "    print(f\"   output_path = {output_path}\")\n",
        "\n",
        "    print(\"\\n[1] Loading corpus...\")\n",
        "    df = load_corpus()\n",
        "    print(f\"    Loaded {len(df)} papers.\")\n",
        "\n",
        "    print(\"\\n[2] Building digest (retrieval + summarization)...\")\n",
        "    digest = build_digest(query, df, top_k=top_k)\n",
        "\n",
        "    print(\"\\n[3] Saving digest JSON...\")\n",
        "    save_digest(digest, output_path)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"\\n Done in {elapsed:.2f} seconds.\")\n",
        "    print(f\"   Papers in digest: {digest['num_results']}\")\n",
        "    print(f\"   JSON saved to:    {output_path}\")\n",
        "\n",
        "    return digest\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Run Day 3 digest pipeline.\")\n",
        "    parser.add_argument(\n",
        "        \"--query\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Search query, e.g. 'transformer models for NLP'\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--top_k\",\n",
        "        type=int,\n",
        "        default=5,\n",
        "        help=\"Number of papers to include in the digest\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        type=str,\n",
        "        default=\"Outputs/pipeline_digest.json\",\n",
        "        help=\"Output path for JSON digest\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run_pipeline(\n",
        "        query=args.query,\n",
        "        top_k=args.top_k,\n",
        "        output_path=args.output,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybUDweHY3Occ",
        "outputId": "719fc66b-5e3b-430f-fa24-52df9a75d2c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"Outputs/transformer_pipeline_digest.json\", \"r\") as f:\n",
        "    digest = json.load(f)\n",
        "\n",
        "print(\"Query:\", digest[\"query\"])\n",
        "print(\"Num results:\", digest[\"num_results\"])\n",
        "print(\"\\nTop 2 paper titles:\")\n",
        "\n",
        "for p in digest[\"papers\"][:2]:\n",
        "    print(f\"- #{p['rank']} {p['title']}\")\n",
        "    if isinstance(p[\"summaries\"], dict):\n",
        "        print(\"  1-sentence summary:\", p[\"summaries\"].get(\"one_sentence\", \"\")[:200])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T30cvYoC3zV5",
        "outputId": "96ff9231-6dea-4f5e-cdb7-dc1094be490e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: transformer models\n",
            "Num results: 3\n",
            "\n",
            "Top 2 paper titles:\n",
            "- #1 Controlling changes to attention logits\n",
            "  1-sentence summary: Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying \n",
            "\n",
            "- #2 IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference\n",
            "  1-sentence summary: Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu rank-bm25 sentence-transformers --quiet"
      ],
      "metadata": {
        "id": "y-lu_j646liF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "print(\"FAISS version:\", faiss.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czF_uKct613Z",
        "outputId": "d3b44307-5abb-4bef-b149-af23ba2f53d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS version: 1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class BM25Retriever:\n",
        "    def __init__(self, corpus=None):\n",
        "        self.tokenized_corpus = None\n",
        "        self.bm25 = None\n",
        "        self.corpus = corpus\n",
        "        if corpus is not None:\n",
        "            self.tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "            self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "        tokenized_query = query.lower().split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(int(idx), float(scores[idx])) for idx in top_indices]\n",
        "\n",
        "\n",
        "class FAISSRetriever:\n",
        "    def __init__(self, embeddings=None, encoder_model=\"all-MiniLM-L6-v2\"):\n",
        "        self.encoder = SentenceTransformer(encoder_model)\n",
        "        self.index = None\n",
        "        self.dimension = None\n",
        "        if embeddings is not None:\n",
        "            self._build_index(embeddings)\n",
        "\n",
        "    def _build_index(self, embeddings):\n",
        "        self.embeddings = embeddings.astype(\"float32\")\n",
        "        self.dimension = self.embeddings.shape[1]\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def load_index(self, index_path):\n",
        "        self.index = faiss.read_index(index_path)\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "        if self.index is None:\n",
        "\n",
        "            return []\n",
        "        query_vec = self.encoder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "        faiss.normalize_L2(query_vec)\n",
        "        scores, indices = self.index.search(query_vec, top_k)\n",
        "        return [(int(idx), float(score)) for idx, score in zip(indices[0], scores[0])]\n",
        "\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(self, bm25_retriever, faiss_retriever, bm25_weight=0.3, semantic_weight=0.7):\n",
        "        self.bm25 = bm25_retriever\n",
        "        self.faiss = faiss_retriever\n",
        "        self.bm25_weight = bm25_weight\n",
        "        self.semantic_weight = semantic_weight\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "        bm25_res = self.bm25.search(query, top_k=50)\n",
        "        faiss_res = self.faiss.search(query, top_k=50)\n",
        "\n",
        "        bm25_scores = {idx: score for idx, score in bm25_res}\n",
        "        faiss_scores = {idx: score for idx, score in faiss_res}\n",
        "\n",
        "        all_indices = set(bm25_scores.keys()) | set(faiss_scores.keys())\n",
        "        if not all_indices:\n",
        "            return []\n",
        "\n",
        "        bm25_max = max(bm25_scores.values()) if bm25_scores else 1.0\n",
        "        faiss_max = max(faiss_scores.values()) if faiss_scores else 1.0\n",
        "\n",
        "        combined = []\n",
        "        for idx in all_indices:\n",
        "            b_score = bm25_scores.get(idx, 0.0) / bm25_max\n",
        "            f_score = faiss_scores.get(idx, 0.0) / faiss_max\n",
        "            final = self.bm25_weight * b_score + self.semantic_weight * f_score\n",
        "            combined.append((idx, final))\n",
        "\n",
        "        combined.sort(key=lambda x: x[1], reverse=True)\n",
        "        return combined[:top_k]\n",
        "\n",
        "\n",
        "class EmbeddingClassifier:\n",
        "    \"\"\"\n",
        "    Wraps Day-2 embedding-based classifier (e.g., LogisticRegression on SBERT).\n",
        "    Expects pickle with {'classifier': model, ...}.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_model=\"all-MiniLM-L6-v2\"):\n",
        "        self.encoder = SentenceTransformer(encoder_model)\n",
        "        self.classifier: LogisticRegression | None = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def attach(self, clf_obj):\n",
        "        if isinstance(clf_obj, dict):\n",
        "            self.classifier = clf_obj[\"classifier\"]\n",
        "            self.classes_ = self.classifier.classes_\n",
        "        else:\n",
        "            self.classifier = clf_obj\n",
        "            self.classes_ = self.classifier.classes_\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        embeddings = self.encoder.encode(texts, show_progress_bar=False)\n",
        "        return self.classifier.predict_proba(embeddings)\n",
        "\n",
        "    def predict_label(self, text: str):\n",
        "        probs = self.predict_proba(text)[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        label = self.classes_[top_idx]\n",
        "        confidence = float(probs[top_idx])\n",
        "        return label, confidence"
      ],
      "metadata": {
        "id": "RqcM3P1U7rUP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def load_retrieval_and_models(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Load BM25, FAISS, Hybrid retriever, classifier, and (optionally) BERTopic model.\n",
        "    Paths are based on your project structure – adjust if needed.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    bm25_path = ROOT / \"data\" / \"retrieval\" / \"bm25_retriever.pkl\"\n",
        "    if bm25_path.exists():\n",
        "        with open(bm25_path, \"rb\") as f:\n",
        "            bm25 = pickle.load(f)\n",
        "        print(\"Loaded BM25 retriever from pickle.\")\n",
        "    else:\n",
        "        print(\"BM25 pickle not found – building BM25 from corpus text...\")\n",
        "        corpus = (\n",
        "            df[\"title\"].fillna(\"\").astype(str)\n",
        "            + \" \"\n",
        "            + df.get(\"original_abstract\", df.get(\"abstract\", \"\")).fillna(\"\").astype(str)\n",
        "        ).tolist()\n",
        "        bm25 = BM25Retriever(corpus)\n",
        "\n",
        "\n",
        "    faiss_index_path = ROOT / \"data\" / \"retrieval\" / \"faiss_index.bin\"\n",
        "    faiss_retriever = FAISSRetriever()\n",
        "    if faiss_index_path.exists():\n",
        "        faiss_retriever.load_index(str(faiss_index_path))\n",
        "        print(\"Loaded FAISS index.\")\n",
        "    else:\n",
        "        emb_path = ROOT / \"data\" / \"embeddings\" / \"sbert_abstract_embeddings.npy\"\n",
        "        if emb_path.exists():\n",
        "            emb = np.load(emb_path)\n",
        "            faiss_retriever._build_index(emb)\n",
        "            print(\"Built FAISS index from sbert_abstract_embeddings.npy.\")\n",
        "        else:\n",
        "            print(\" No FAISS index or embeddings found – semantic retrieval will be skipped.\")\n",
        "\n",
        "    hybrid_retriever = HybridRetriever(bm25, faiss_retriever)\n",
        "\n",
        "    clf_path = ROOT / \"models\" / \"embedding_classifier.pkl\"\n",
        "    classifier_wrapper = None\n",
        "    if clf_path.exists():\n",
        "        classifier_wrapper = EmbeddingClassifier()\n",
        "        with open(clf_path, \"rb\") as f:\n",
        "            clf_obj = pickle.load(f)\n",
        "        classifier_wrapper.attach(clf_obj)\n",
        "        print(\"Loaded embedding classifier.\")\n",
        "    else:\n",
        "        print(\" No classifier pickle found – classifier labels will be None.\")\n",
        "\n",
        "\n",
        "    topic_model = None\n",
        "    topic_model_path = ROOT / \"models\" / \"bertopic_model.pkl\"\n",
        "    if topic_model_path.exists():\n",
        "        try:\n",
        "            from bertopic import BERTopic\n",
        "            topic_model = BERTopic.load(topic_model_path)\n",
        "            print(\"Loaded BERTopic topic model.\")\n",
        "        except Exception as e:\n",
        "            print(f\" Could not load BERTopic model: {e}\")\n",
        "\n",
        "    return hybrid_retriever, classifier_wrapper, topic_model"
      ],
      "metadata": {
        "id": "QOHwTKiq8QkL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any\n",
        "\n",
        "def build_digest_full(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    top_k: int,\n",
        "    hybrid_retriever,\n",
        "    classifier_wrapper: EmbeddingClassifier | None,\n",
        "    topic_model,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Build a full digest:\n",
        "      - Hybrid retrieval (BM25 + FAISS)\n",
        "      - Summarization\n",
        "      - Classifier prediction\n",
        "      - Topic modeling (if available)\n",
        "    \"\"\"\n",
        "    results = hybrid_retriever.search(query, top_k=top_k)\n",
        "    indices = [idx for idx, _ in results]\n",
        "\n",
        "    if not indices:\n",
        "        print(\"No retrieval hits – falling back to first top_k papers.\")\n",
        "        indices = df.index.tolist()[:top_k]\n",
        "\n",
        "    papers_out = []\n",
        "\n",
        "    for rank, idx in enumerate(indices, start=1):\n",
        "        row = df.iloc[idx]\n",
        "        title = row.get(\"title\", \"Untitled\")\n",
        "        abstract = row.get(\"original_abstract\", row.get(\"abstract\", \"\")) or \"\"\n",
        "\n",
        "\n",
        "        summaries = generate_summaries(str(abstract))\n",
        "\n",
        "\n",
        "        pred_label, pred_conf = None, None\n",
        "        if classifier_wrapper is not None and classifier_wrapper.classifier is not None:\n",
        "            text_for_cls = f\"{title} {abstract}\"\n",
        "            pred_label, pred_conf = classifier_wrapper.predict_label(text_for_cls)\n",
        "\n",
        "\n",
        "        topic_id, topic_label = None, None\n",
        "        if topic_model is not None:\n",
        "            try:\n",
        "                topics, _ = topic_model.transform([abstract])\n",
        "                topic_id = int(topics[0])\n",
        "                topic_label = str(topic_id)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        papers_out.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"paper_id\": row.get(\"paper_id\", int(idx)),\n",
        "                \"title\": title,\n",
        "                \"venue\": row.get(\"venue\"),\n",
        "                \"year\": int(row[\"year\"]) if \"year\" in row and pd.notna(row[\"year\"]) else None,\n",
        "                \"summaries\": summaries,\n",
        "                \"classifier_label\": pred_label,\n",
        "                \"classifier_confidence\": pred_conf,\n",
        "                \"topic_id\": topic_id,\n",
        "                \"topic_label\": topic_label,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"num_results\": len(papers_out),\n",
        "        \"papers\": papers_out,\n",
        "    }"
      ],
      "metadata": {
        "id": "7ufZsQ-d88Ij"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(\n",
        "    query: str,\n",
        "    top_k: int = 5,\n",
        "    output_path: str = \"Outputs/pipeline_digest.json\",\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline:\n",
        "      1. Load corpus\n",
        "      2. Load BM25 / FAISS / classifier / topic model\n",
        "      3. Hybrid retrieval\n",
        "      4. Summarization\n",
        "      5. Add classifier + topic info\n",
        "      6. Save JSON\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    print(f\" Running pipeline for query: '{query}'\")\n",
        "    print(f\"   top_k       = {top_k}\")\n",
        "    print(f\"   output_path = {output_path}\")\n",
        "\n",
        "    print(\"\\n[1] Loading corpus...\")\n",
        "    df = load_corpus()\n",
        "    print(f\"    Loaded {len(df)} papers.\")\n",
        "\n",
        "    print(\"\\n[2] Loading retrieval and models...\")\n",
        "    hybrid_retriever, classifier_wrapper, topic_model = load_retrieval_and_models(df)\n",
        "\n",
        "    print(\"\\n[3] Building full digest...\")\n",
        "    digest = build_digest_full(\n",
        "        query=query,\n",
        "        df=df,\n",
        "        top_k=top_k,\n",
        "        hybrid_retriever=hybrid_retriever,\n",
        "        classifier_wrapper=classifier_wrapper,\n",
        "        topic_model=topic_model,\n",
        "    )\n",
        "\n",
        "    print(\"\\n[4] Saving digest JSON...\")\n",
        "    save_digest(digest, output_path)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"\\n Done in {elapsed:.2f} seconds.\")\n",
        "    print(f\"   Papers in digest: {digest['num_results']}\")\n",
        "    print(f\"   JSON saved to:    {output_path}\")\n",
        "\n",
        "    return digest"
      ],
      "metadata": {
        "id": "x_iRiIL09UjS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Final-Project-Group-LexiCore\n",
        "\n",
        "!python run_pipeline.py --query \"transformer models\" --top_k 5 --output Outputs/full_hybrid_pipeline_digest.json\n",
        "\n",
        "!ls Outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6Um6H4g-KBW",
        "outputId": "f8b288f3-1ff0-4496-ad8a-1991e1f76dcc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Final-Project-Group-LexiCore\n",
            "2025-12-02 21:32:00.169935: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764711120.193633    9646 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764711120.200636    9646 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764711120.218089    9646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764711120.218132    9646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764711120.218136    9646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764711120.218140    9646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            " Running pipeline for query: 'transformer models'\n",
            "   top_k       = 5\n",
            "   output_path = Outputs/full_hybrid_pipeline_digest.json\n",
            "\n",
            "[1] Loading corpus...\n",
            "Trying to load: /content/Final-Project-Group-LexiCore/data/processed/cleaned_paper.parquet\n",
            "Trying to load: /content/Final-Project-Group-LexiCore/data/processed/cleaned_papers.parquet\n",
            "Loaded 500 rows from cleaned_papers.parquet\n",
            "    Loaded 500 papers.\n",
            "\n",
            "[2] Building digest (retrieval + summarization)...\n",
            "\n",
            "[3] Saving digest JSON...\n",
            "\n",
            " Done in 0.39 seconds.\n",
            "   Papers in digest: 3\n",
            "   JSON saved to:    Outputs/full_hybrid_pipeline_digest.json\n",
            "full_hybrid_pipeline_digest.json  transformer_pipeline_digest.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"Outputs/full_hybrid_pipeline_digest.json\") as f:\n",
        "    digest = json.load(f)\n",
        "\n",
        "print(digest[\"query\"], \"→\", digest[\"num_results\"], \"papers\")\n",
        "print(digest[\"papers\"][0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7fAkZ0v-zDD",
        "outputId": "0e765ec4-a6c0-4218-8abe-73dcfba4133e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer models → 3 papers\n",
            "dict_keys(['rank', 'paper_id', 'title', 'venue', 'year', 'summaries'])\n"
          ]
        }
      ]
    }
  ]
}